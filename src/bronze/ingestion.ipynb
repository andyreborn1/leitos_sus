{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca0ea6c4-ee25-4ef4-bbdd-6bbde0126fea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "table_name = dbutils.widgets.get(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03ccb361-30ae-4546-a7ef-2245e628f438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#catalog = \"bronze\"\n",
    "#schema = \"sistema_leitos\"\n",
    "#table_name = \"leitos\"\n",
    "#table_id = \"comp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98218f68-11dd-4cf9-a0e1-5c7cdf671a73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def table_exists(catalog, schema, tablename):\n",
    "    query = f\"show tables from {catalog}.{schema}\"\n",
    "    table_ok = spark.sql(query).filter(f\"tableName = '{tablename}'\").count()\n",
    "\n",
    "    return table_ok == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cf9cfcc-bbf5-4a0c-b58b-5d378b67960c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_delta_table(\n",
    "    df, catalog, schema, table_name, on_condition, columns_to_update\n",
    "):\n",
    "    # spark.catalog.tableExists(\"bronze.sistema_leitos.hospitais\") check if column exist\n",
    "    if not table_exists(catalog, schema, table_name):\n",
    "        print(f\"Criando tabela {catalog}.{schema}.{table_name}!!!\")\n",
    "        (\n",
    "            df.coalesce(1)\n",
    "            .write.format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .saveAsTable(f\"{catalog}.{schema}.{table_name}\")\n",
    "        )\n",
    "\n",
    "        return\n",
    "\n",
    "    from delta.tables import DeltaTable\n",
    "\n",
    "    print(f\"Atualizando tabela {catalog}.{schema}.{table_name}!!!\")\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, f\"{catalog}.{schema}.{table_name}\")\n",
    "\n",
    "    (\n",
    "        delta_table.alias(\"target\")\n",
    "        .merge(df.alias(\"source\"), on_condition)\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"target.row_hash != source.row_hash\",\n",
    "            set={\n",
    "                **{c: f\"source.{c}\" for c in columns_to_update},\n",
    "                \"updated_at\": \"current_timestamp()\",\n",
    "            },\n",
    "        )\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    sumarry = (\n",
    "        delta_table.history(1)\n",
    "        .select(\n",
    "            \"operationMetrics.numTargetRowsInserted\",\n",
    "            \"operationMetrics.numTargetRowsUpdated\",\n",
    "            \"operationMetrics.numTargetRowsDeleted\",\n",
    "        )\n",
    "        .collect()[0]\n",
    "    )\n",
    "\n",
    "    print(f\"Linhas inseridas: {sumarry['numTargetRowsInserted']}\")\n",
    "    print(f\"Linhas atualizadas: {sumarry['numTargetRowsUpdated']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fff0f18-5544-469b-8483-4d6bbd1b0395",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ingestão full load"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(\n",
    "        header=True,\n",
    "        delimiter=\",\",\n",
    "        inferSchema=True,\n",
    "        encoding=\"latin1\",\n",
    "    )\n",
    "    .load(\"/Volumes/raw-data/sistema_leitos/leitos/\")\n",
    ")\n",
    "\n",
    "df = df.withColumnsRenamed({y: re.sub(r\"\\W+\", \"_\", y).lower() for y in df.columns})\n",
    "\n",
    "cols_to_hash = df.columns[20:]\n",
    "\n",
    "df = df.withColumns(\n",
    "    {\n",
    "        \"row_hash\": F.sha2(\n",
    "            F.concat_ws(\n",
    "                \"||\",\n",
    "                *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in cols_to_hash],\n",
    "            ),\n",
    "            256,\n",
    "        ),\n",
    "        \"updated_at\": F.current_timestamp(),\n",
    "    }\n",
    ")\n",
    "\n",
    "on_condition = f\"target.comp = source.comp and target.cnes = source.cnes\"\n",
    "\n",
    "upsert_delta_table(df, catalog, schema, table_name, on_condition, cols_to_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d201073-8846-445a-9171-1ba1e9c186af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Como fazer a ingestão incremental em caso de CDC:\n",
    "## 1 - Ler todos os arquivos do CDC\n",
    "`spark.read.format(\"extensao\").options(add_option).load(\"path_dos_arquivos\")`\n",
    "## 2 - Criar deduplicação baseado no id para pegar o arquivo mais recente\n",
    "Ex: ordernar pela data de alteração e fazer um unique mantendo o primeiro registro\n",
    "## 3 - Carregar a tabela delta atual\n",
    "```\n",
    "import delta\n",
    "\n",
    "delta.DeltaTable.ForName(spark, \"schema.nome_tabela\")\n",
    "```\n",
    "## 4 - Fazer o UPSERT\n",
    "merge da tabela atual com as novas informações designando o que fazer para cada tipo de operação\n",
    "```\n",
    "tab_atual.alias(\"ta\")\n",
    ".merge(tab_nova.alias(\"tn\"), \"ta.id = tn.id\")\n",
    ".whenMatchedDelete(condition = \"tn.operation = 'D'\")\n",
    ".whenMatchedUpdateAll(condition = \"tn.operation = 'U'\")\n",
    ".whenMatchedInsertAll(condition = \"tn.operation = 'i'\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7148141948863509,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
