{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca0ea6c4-ee25-4ef4-bbdd-6bbde0126fea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "table_name = dbutils.widgets.get(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98218f68-11dd-4cf9-a0e1-5c7cdf671a73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def table_exists(catalog, schema, tablename):\n",
    "    query = f\"show tables from {catalog}.{schema}\"\n",
    "    table_ok = spark.sql(query).filter(f\"tableName = '{tablename}'\").count()\n",
    "\n",
    "    return table_ok == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fff0f18-5544-469b-8483-4d6bbd1b0395",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ingestão full load"
    }
   },
   "outputs": [],
   "source": [
    "if not table_exists(catalog, schema, table_name):\n",
    "    df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .options(\n",
    "            header=True,\n",
    "            delimiter=\",\",\n",
    "            inferSchema=True,\n",
    "            encoding=\"latin1\",\n",
    "        )\n",
    "        .load(\"/Volumes/raw-data/sistema_leitos/leitos/\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumnsRenamed({y: y.lower().replace(r\" \", \"_\") for y in df.columns})\n",
    "\n",
    "    (\n",
    "        df.coalesce(1)\n",
    "        .write.format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(f\"{catalog}.{schema}.{table_name}\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d201073-8846-445a-9171-1ba1e9c186af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Como fazer a ingestão incremental em caso de CDC:\n",
    "## 1 - Ler todos os arquivos do CDC\n",
    "`spark.read.format(\"extensao\").options(add_option).load(\"path_dos_arquivos\")`\n",
    "## 2 - Criar deduplicação baseado no id para pegar o arquivo mais recente\n",
    "Ex: ordernar pela data de alteração e fazer um unique mantendo o primeiro registro\n",
    "## 3 - Carregar a tabela delta atual\n",
    "```\n",
    "import delta\n",
    "\n",
    "delta.DeltaTable.ForName(spark, \"schema.nome_tabela\")\n",
    "```\n",
    "## 4 - Fazer o UPSERT\n",
    "merge da tabela atual com as novas informações designando o que fazer para cada tipo de operação\n",
    "```\n",
    "tab_atual.alias(\"ta\")\n",
    ".merge(tab_nova.alias(\"tn\"), \"ta.id = tn.id\")\n",
    ".whenMatchedDelete(condition = \"tn.operation = 'D'\")\n",
    ".whenMatchedUpdateAll(condition = \"tn.operation = 'U'\")\n",
    ".whenMatchedInsertAll(condition = \"tn.operation = 'i'\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "705215a2-9a0a-4350-be7f-d5d38e926cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.tableExists(\"bronze.sistema_leitos.hospitais\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8316234122801961,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
